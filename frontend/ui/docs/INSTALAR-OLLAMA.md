# ğŸ¤– Instalar Ollama - IA Local 100% Gratis

## ğŸ¯ Â¿Por quÃ© Ollama?

âœ… **100% GRATIS** - Sin lÃ­mites, sin API keys
âœ… **PRIVADO** - Tus datos nunca salen de tu PC
âœ… **RÃPIDO** - Corre localmente, sin depender de internet
âœ… **POTENTE** - Modelos como Llama 3, Mistral, Gemma
âœ… **FÃCIL** - InstalaciÃ³n en 2 minutos

---

## ğŸ“¥ InstalaciÃ³n (Windows)

### Paso 1: Descargar Ollama

**Visita:** https://ollama.com/download

Descarga el instalador para Windows (OllamaSetup.exe)

### Paso 2: Instalar

1. Ejecuta `OllamaSetup.exe`
2. Sigue el asistente (Next, Next, Install)
3. Espera 1-2 minutos

### Paso 3: Verificar InstalaciÃ³n

Abre PowerShell y ejecuta:

```powershell
ollama --version
```

DeberÃ­as ver algo como: `ollama version 0.x.x`

---

## ğŸš€ Descargar Modelo de IA

Ollama necesita descargar un modelo de IA la primera vez.

**Modelo recomendado para tu proyecto:**

```powershell
ollama pull llama3.2:3b
```

Este comando descarga Llama 3.2 (3 mil millones de parÃ¡metros):
- **TamaÃ±o:** ~2GB
- **Velocidad:** Muy rÃ¡pido
- **Calidad:** Excelente para educaciÃ³n
- **Requisitos:** 8GB RAM mÃ­nimo

### Modelos alternativos:

#### Si tu PC tiene poca RAM (4-8GB):
```powershell
ollama pull phi3:mini
```
- TamaÃ±o: 2.3GB
- MÃ¡s ligero y rÃ¡pido

#### Si quieres mÃ¡s calidad (PC potente, 16GB+ RAM):
```powershell
ollama pull llama3.2:7b
```
- TamaÃ±o: 4.7GB
- Respuestas mÃ¡s detalladas

---

## âœ… Probar que Funciona

### OpciÃ³n 1: Desde PowerShell

```powershell
ollama run llama3.2:3b
```

Escribe una pregunta en inglÃ©s:
```
>>> What is space weather?
```

Si responde, Â¡funciona! Presiona `Ctrl + D` para salir.

### OpciÃ³n 2: Desde tu Backend

```powershell
cd C:\Users\docta\Desktop\Github\SpaceApp-NASA\backend
node test-ollama.js
```

---

## ğŸ”§ ConfiguraciÃ³n AutomÃ¡tica

Ollama corre automÃ¡ticamente en segundo plano despuÃ©s de instalarse.

**Puerto por defecto:** `http://localhost:11434`

**Verifica que estÃ© corriendo:**

```powershell
curl http://localhost:11434/api/tags
```

Si responde con JSON, estÃ¡ funcionando.

---

## ğŸ¯ IntegraciÃ³n con tu Proyecto

Tu backend **YA ESTÃ CONFIGURADO** para usar Ollama:

1. **Prioridad 1:** Intenta Ollama (local, gratis)
2. **Prioridad 2:** Si Ollama no estÃ¡, usa Gemini (cloud)

### Flujo automÃ¡tico:

```
Usuario pregunta
    â†“
Backend intenta Ollama (localhost:11434)
    â†“
Â¿Ollama responde?
    SÃ â†’ Usa respuesta de Ollama âœ…
    NO â†’ Intenta Gemini (si hay API key) âœ…
```

---

## ğŸš€ CÃ³mo Usar

### 1. AsegÃºrate que Ollama estÃ© corriendo

```powershell
# Verificar servicio
Get-Process ollama

# Si no estÃ¡ corriendo, iniciarlo
ollama serve
```

### 2. Inicia tu backend

```powershell
cd backend
npm run dev
```

### 3. Usa el asistente

Abre `frontend/ui/html/index.html` y haz preguntas al chatbot.

---

## ğŸ“Š ComparaciÃ³n de Opciones

| CaracterÃ­stica | Ollama (Local) | Gemini (Cloud) |
|----------------|----------------|----------------|
| **Costo** | 100% Gratis | Gratis (lÃ­mites) |
| **Velocidad** | Muy rÃ¡pido | Depende de internet |
| **Privacidad** | 100% privado | Datos en la nube |
| **LÃ­mites** | Sin lÃ­mites | 15 req/min |
| **Requisitos** | 8GB RAM | API key |
| **Internet** | No necesario | Necesario |
| **Setup** | 5 minutos | 2 minutos |

---

## ğŸ’¡ Comandos Ãštiles de Ollama

### Ver modelos instalados:
```powershell
ollama list
```

### Descargar otro modelo:
```powershell
ollama pull mistral
ollama pull gemma2
```

### Eliminar un modelo (liberar espacio):
```powershell
ollama rm llama3.2:3b
```

### Ver uso de memoria:
```powershell
ollama ps
```

### Detener Ollama:
```powershell
ollama stop
```

---

## ğŸ› SoluciÃ³n de Problemas

### "ollama: command not found"

Reinicia PowerShell despuÃ©s de instalar.

### "connection refused"

Ollama no estÃ¡ corriendo. Ejecuta:
```powershell
ollama serve
```

### Respuestas muy lentas

Tu modelo es muy pesado para tu PC. Usa uno mÃ¡s ligero:
```powershell
ollama pull phi3:mini
```

Y actualiza el modelo en `backend/services/geminiAPI.js` lÃ­nea 9:
```javascript
const OLLAMA_MODEL = 'phi3:mini';
```

### Error de memoria

Tu PC no tiene suficiente RAM. Opciones:
1. Cierra otras aplicaciones
2. Usa un modelo mÃ¡s pequeÃ±o (`phi3:mini`)
3. Usa Gemini como fallback

---

## âœ… Checklist de InstalaciÃ³n

- [ ] Ollama descargado desde https://ollama.com/download
- [ ] Ollama instalado
- [ ] Comando `ollama --version` funciona
- [ ] Modelo descargado: `ollama pull llama3.2:3b`
- [ ] Ollama corriendo: `ollama serve`
- [ ] Backend reiniciado
- [ ] Asistente probado y funcionando

---

## ğŸ“ Para el Concurso NASA

**Menciona esto en tu presentaciÃ³n:**

> "Nuestro asistente educativo usa **Ollama**, una IA de cÃ³digo abierto que corre localmente, garantizando privacidad y sin costos. Como fallback, tambiÃ©n soporta Google Gemini en la nube."

**Ventajas para el concurso:**
- âœ… Sin dependencias de servicios externos
- âœ… Funciona sin internet (despuÃ©s de instalar)
- âœ… 100% gratuito y escalable
- âœ… Privacidad de datos garantizada

---

## ğŸš€ PrÃ³ximos Pasos

1. âœ… Instala Ollama
2. âœ… Descarga el modelo (`llama3.2:3b`)
3. âœ… Reinicia el backend
4. âœ… Prueba el asistente
5. âœ… Â¡Listo para el concurso! ğŸŒŸ

---

## ğŸ“š Recursos

- **Web oficial:** https://ollama.com
- **Modelos disponibles:** https://ollama.com/library
- **DocumentaciÃ³n:** https://github.com/ollama/ollama/blob/main/docs/api.md

---

Â¿Problemas? Comparte el error especÃ­fico y te ayudo. ğŸ¤
